# Mixture-of-Agents: Enhancing Large Language Model Capabilities (2024)

**Authors**: Together AI & Others

## ğŸ“ Summary
This paper proposes the **Mixture-of-Agents (MoA)** methodology, demonstrating that a collaborative system of multiple LLMs can achieve performance superior to any single model in the system. It leverages the "collaborativeness" of LLMsâ€”their ability to improve their own responses when presented with outputs from other models.

## ğŸ”‘ Key Concepts
- **Layered Architecture**: Agents are arranged in layers, where agents in the next layer take inputs from all agents in the previous layer.
- **Collaborativeness**: The phenomenon where an LLM generates better output when conditioned on good responses from other models.
- **Performance**: MoA achieved state-of-the-art results on benchmarks like AlpacaEval 2.0, surpassing GPT-4o in certain configurations.

## ğŸ”— Links
- [Paper (arXiv)](https://arxiv.org/abs/2406.04692)
- [Together AI Blog](https://www.together.ai/blog/mixture-of-agents)
