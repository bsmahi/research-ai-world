# Attention Is All You Need (2017)

**Authors**: Ashish Vaswani et al. (Google Brain)

## ğŸ“ Summary
This seminal paper introduced the **Transformer** architecture, which dispensed with recurrence and convolutions entirely, relying solely on attention mechanisms. This architecture became the foundation for all modern Large Language Models (LLMs) like GPT, BERT, and Claude.

## ğŸ”‘ Key Concepts
- **Self-Attention**: Mechanism relating different positions of a single sequence to compute a representation of the sequence.
- **Multi-Head Attention**: Allows the model to jointly attend to information from different representation subspaces at different positions.
- **Positional Encoding**: Injects information about the relative or absolute position of the tokens in the sequence.

## ğŸ”— Links
- [Paper (arXiv)](https://arxiv.org/abs/1706.03762)
